defaults:
  - _self_
  - datamodule: 8M_1M_opt
  
# Model configuration
expansion_factor: 32

# Directories
eval_seq_path: null #reticular-sae/eval/data/cath_seqs.csv

# Training parameters
epochs: 1
batch_size: 2048 #TODO: Rename this to specify that it's the per-gpu token-based batch size
seed: 0

# Evaluation settings
eval_batch_size: 32
eval_steps: 10000
eval_start_step: 10000

# Weights & Biases logging
use_wandb: false
wandb_entity: "your-wandb-entity"
wandb_project: "your-wandb-project"
wandb_name: "your-wandb-name"

# Checkpointing
save_dir: "ckpts/"
# max_ckpts_to_keep: 3
save_steps: 100000
log_steps: 100
ckpt_path: null

# Debugging
fast_dev_run: false
watch_steps_locally: false
computed: null